#!/usr/bin/env python3
"""
å¾®åšé“¾æ¥æå–å™¨
åŠŸèƒ½ï¼šä»å¾®åšé“¾æ¥ä¸­æå–å¸–å­å†…å®¹å¹¶æ ¼å¼åŒ–è¾“å‡º
æ”¯æŒï¼šweibo.comã€m.weibo.cn ç­‰é“¾æ¥
"""

import re
import json
import argparse
from typing import List, Dict, Optional
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import requests
from bs4 import BeautifulSoup


class WeiboPost:
    """å¾®åšå¸–å­æ•°æ®æ¨¡å‹"""
    
    def __init__(self, data: Dict):
        self.url = data.get('url', '')
        self.text = data.get('text', '')
        self.author = data.get('author', '')
        self.created_at = data.get('created_at', '')
        self.reposts_count = data.get('reposts_count', 0)
        self.comments_count = data.get('comments_count', 0)
        self.attitudes_count = data.get('attitudes_count', 0)
        self.pics = data.get('pics', [])
        self.topics = data.get('topics', [])
        self.links = data.get('links', [])
    
    def format_output(self, format_type: str = 'markdown') -> str:
        """æ ¼å¼åŒ–è¾“å‡º"""
        if format_type == 'markdown':
            return self._format_markdown()
        elif format_type == 'json':
            return self._format_json()
        else:
            return self._format_plain()
    
    def _format_markdown(self) -> str:
        """Markdown æ ¼å¼è¾“å‡º"""
        output = []
        output.append(f"## {self.author} çš„å¾®åš")
        output.append(f"**å‘å¸ƒæ—¶é—´**: {self.created_at}")
        output.append(f"**åŸæ–‡é“¾æ¥**: {self.url}")
        output.append("")
        output.append("### å†…å®¹")
        output.append(self.text)
        output.append("")
        
        if self.topics:
            output.append(f"**è¯é¢˜**: {', '.join(self.topics)}")
            output.append("")
        
        if self.links:
            output.append("### å¼•ç”¨é“¾æ¥")
            for link in self.links:
                output.append(f"- {link}")
            output.append("")
        
        if self.pics:
            output.append(f"**å›¾ç‰‡**: {len(self.pics)} å¼ ")
            output.append("")
        
        output.append("### äº’åŠ¨æ•°æ®")
        output.append(f"- è½¬å‘: {self.reposts_count}")
        output.append(f"- è¯„è®º: {self.comments_count}")
        output.append(f"- ç‚¹èµ: {self.attitudes_count}")
        output.append("")
        output.append("---")
        
        return "\n".join(output)
    
    def _format_json(self) -> str:
        """JSON æ ¼å¼è¾“å‡º"""
        data = {
            'url': self.url,
            'author': self.author,
            'text': self.text,
            'created_at': self.created_at,
            'topics': self.topics,
            'links': self.links,
            'pics': self.pics,
            'metrics': {
                'reposts': self.reposts_count,
                'comments': self.comments_count,
                'likes': self.attitudes_count
            }
        }
        return json.dumps(data, ensure_ascii=False, indent=2)
    
    def _format_plain(self) -> str:
        """çº¯æ–‡æœ¬æ ¼å¼è¾“å‡º"""
        output = []
        output.append(f"ä½œè€…: {self.author}")
        output.append(f"æ—¶é—´: {self.created_at}")
        output.append(f"é“¾æ¥: {self.url}")
        output.append("-" * 50)
        output.append(self.text)
        if self.topics:
            output.append(f"\nè¯é¢˜: {', '.join(self.topics)}")
        if self.links:
            output.append(f"\nå¤–é“¾: {', '.join(self.links)}")
        output.append(f"\nè½¬å‘:{self.reposts_count} | è¯„è®º:{self.comments_count} | ç‚¹èµ:{self.attitudes_count}")
        output.append("=" * 50)
        return "\n".join(output)


class WeiboExtractor:
    """å¾®åšå†…å®¹æå–å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) '
                         'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cookie': 'SUB=_2AkMSbR7af8NxqwJRmP0SzGvhZY11yQ_EieKkjJ2ZJRMxHRl-yT83qkEctRB6PfaHqS4h4R4q4r4q4r4q4r4q4r4q; M_WEIBOCN_PARAMS=oid%3D4999999999999999%26luicode%3D20000174%26uicode%3D20000174;',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def extract(self, url: str) -> Optional[WeiboPost]:
        """æå–å¾®åšå†…å®¹"""
        try:
            # è¯†åˆ«é“¾æ¥ç±»å‹å¹¶è§„èŒƒåŒ–
            normalized_url = self._normalize_url(url)
            if not normalized_url:
                print(f"âŒ æ— æ³•è¯†åˆ«çš„å¾®åšé“¾æ¥: {url}")
                return None
            
            print(f"ğŸ” æ­£åœ¨æå–: {normalized_url}")
            
            # è·å–ç½‘é¡µå†…å®¹
            response = self.session.get(normalized_url, timeout=10)
            response.raise_for_status()
            
            # è§£æå†…å®¹
            post_data = self._parse_mobile_page(response.text, normalized_url)
            
            if post_data:
                print(f"âœ… æˆåŠŸæå–: {post_data.get('author', 'æœªçŸ¥')}")
                return WeiboPost(post_data)
            else:
                print(f"âš ï¸ æœªèƒ½æå–åˆ°å†…å®¹")
                return None
                
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            return None
    
    def _normalize_url(self, url: str) -> Optional[str]:
        """è§„èŒƒåŒ–å¾®åšé“¾æ¥"""
        # æå–å¾®åš ID
        patterns = [
            r'weibo\.com/\d+/(\w+)',           # weibo.com/uid/mid
            r'm\.weibo\.cn/status/(\d+)',      # m.weibo.cn/status/mid
            r'm\.weibo\.cn/\d+/(\w+)',         # m.weibo.cn/uid/mid
            r'weibo\.cn/\w+/(\w+)',            # weibo.cn/username/mid
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                mid = match.group(1)
                # è½¬æ¢ä¸ºç§»åŠ¨ç‰ˆé“¾æ¥ï¼ˆæ›´å®¹æ˜“è§£æï¼‰
                return f"https://m.weibo.cn/status/{mid}"
        
        return None
    
    def _parse_mobile_page(self, html: str, url: str) -> Optional[Dict]:
        """è§£æç§»åŠ¨ç‰ˆç½‘é¡µ"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°è¯•ä»é¡µé¢ä¸­æå– JSON æ•°æ®
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string and 'var $render_data' in script.string:
                # æå– JSON æ•°æ®
                json_match = re.search(r'\$render_data = (\[.*?\])\[0\]', script.string, re.DOTALL)
                if json_match:
                    try:
                        data = json.loads(json_match.group(1))[0]
                        return self._parse_json_data(data, url)
                    except:
                        pass
        
        # å¦‚æœ JSON æå–å¤±è´¥ï¼Œå°è¯• HTML è§£æ
        return self._parse_html_fallback(soup, url)
    
    def _parse_json_data(self, data: Dict, url: str) -> Dict:
        """ä» JSON æ•°æ®ä¸­æå–ä¿¡æ¯"""
        status = data.get('status', {})
        user = status.get('user', {})
        
        # æå–æ–‡æœ¬å†…å®¹
        text = status.get('text', '')
        # æ¸…ç† HTML æ ‡ç­¾
        text = re.sub(r'<br\s*/?>', '\n', text)
        text = BeautifulSoup(text, 'html.parser').get_text()
        
        # æå–è¯é¢˜
        topics = re.findall(r'#([^#]+)#', text)
        
        # æå–é“¾æ¥
        links = []
        url_struct = status.get('url_struct', [])
        for url_info in url_struct:
            if 'long_url' in url_info:
                links.append(url_info['long_url'])
        
        # æå–å›¾ç‰‡
        pics = []
        if 'pics' in status:
            pics = [pic.get('large', {}).get('url', '') for pic in status['pics']]
        
        return {
            'url': url,
            'text': text.strip(),
            'author': user.get('screen_name', 'æœªçŸ¥'),
            'created_at': status.get('created_at', ''),
            'reposts_count': status.get('reposts_count', 0),
            'comments_count': status.get('comments_count', 0),
            'attitudes_count': status.get('attitudes_count', 0),
            'pics': pics,
            'topics': topics,
            'links': links
        }
    
    def _parse_html_fallback(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """HTML è§£æå¤‡ç”¨æ–¹æ¡ˆ"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¤‡ç”¨è§£ææ–¹æ¡ˆ
        # å®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ ¹æ®å¾®åšé¡µé¢ç»“æ„è°ƒæ•´
        
        text_elem = soup.find('div', class_='weibo-text')
        author_elem = soup.find('div', class_='m-text-cut')
        
        if not text_elem:
            return None
        
        text = text_elem.get_text().strip()
        author = author_elem.get_text().strip() if author_elem else 'æœªçŸ¥'
        
        return {
            'url': url,
            'text': text,
            'author': author,
            'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'reposts_count': 0,
            'comments_count': 0,
            'attitudes_count': 0,
            'pics': [],
            'topics': re.findall(r'#([^#]+)#', text),
            'links': []
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¾®åšé“¾æ¥æå–å™¨ - ä»å¾®åšé“¾æ¥ä¸­æå–å¸–å­å†…å®¹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æå–å•ä¸ªé“¾æ¥
  python weibo_extractor.py https://weibo.com/1234567890/AbCdEfG

  # æå–å¤šä¸ªé“¾æ¥
  python weibo_extractor.py url1 url2 url3

  # æŒ‡å®šè¾“å‡ºæ ¼å¼
  python weibo_extractor.py --format json url1

  # ä¿å­˜åˆ°æ–‡ä»¶
  python weibo_extractor.py --output result.md url1
        """
    )
    
    parser.add_argument(
        'urls',
        nargs='+',
        help='ä¸€ä¸ªæˆ–å¤šä¸ªå¾®åšé“¾æ¥'
    )
    
    parser.add_argument(
        '-f', '--format',
        choices=['markdown', 'json', 'plain'],
        default='markdown',
        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: markdown)'
    )
    
    parser.add_argument(
        '-o', '--output',
        help='è¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆä¸æŒ‡å®šåˆ™è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæå–å™¨
    extractor = WeiboExtractor()
    
    # æå–æ‰€æœ‰é“¾æ¥
    posts = []
    for url in args.urls:
        post = extractor.extract(url)
        if post:
            posts.append(post)
    
    if not posts:
        print("\nâš ï¸ æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•å†…å®¹")
        return
    
    # æ ¼å¼åŒ–è¾“å‡º
    output_lines = []
    if args.format == 'json':
        # JSON æ ¼å¼è¾“å‡ºæ‰€æœ‰å¸–å­
        all_data = [json.loads(post.format_output('json')) for post in posts]
        output_content = json.dumps(all_data, ensure_ascii=False, indent=2)
    else:
        # Markdown æˆ– Plain æ ¼å¼
        header = f"# å¾®åšæå–ç»“æœ\n\næå–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\næå–æ•°é‡: {len(posts)} æ¡\n\n"
        output_lines.append(header)
        
        for i, post in enumerate(posts, 1):
            output_lines.append(post.format_output(args.format))
            if i < len(posts):
                output_lines.append("\n")
        
        output_content = "".join(output_lines)
    
    # è¾“å‡ºç»“æœ
    if args.output:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_content)
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    else:
        print("\n" + "="*60)
        print(output_content)
        print("="*60)


if __name__ == '__main__':
    main()
