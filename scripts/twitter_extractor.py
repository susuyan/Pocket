#!/usr/bin/env python3
"""
X/Twitter å†…å®¹æå–å™¨ (Robust Version)
åŠŸèƒ½ï¼šä» X/Twitter é“¾æ¥ä¸­æå–å¸–å­å†…å®¹ï¼ˆä»…å†…å®¹ï¼‰
ç‰¹æ€§ï¼š
1. ä¼˜å…ˆä½¿ç”¨ Guest Token ç›´æ¥è®¿é—®å®˜æ–¹ API (æœ€ç¨³å®š)
2. è‡ªåŠ¨è½®è¯¢å¤šä¸ª Nitter å®ä¾‹ä½œä¸ºå¤‡é€‰
3. è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿä»£ç†
4. è¾“å‡ºç®€æ´ï¼Œä»…åŒ…å«å†…å®¹
"""

import re
import json
import argparse
import os
import sys
import time
from typing import List, Dict, Optional
from datetime import datetime
import requests
from bs4 import BeautifulSoup

class TwitterExtractor:
    # Twitter Web Client Bearer Token (å›ºå®šå€¼)
    GUEST_BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA"

    # Nitter å®ä¾‹åˆ—è¡¨ (æŒ‰ç¨³å®šæ€§æ’åº)
    NITTER_INSTANCES = [
        "https://nitter.privacydev.net",
        "https://nitter.poast.org",
        "https://nitter.lucabased.xyz",
        "https://nitter.net",
        "https://nitter.cz",
        "https://nitter.projectsegfau.lt",
        "https://nitter.eu.projectsegfau.lt",
        "https://nitter.moomoo.me",
        "https://nitter.soopy.moe",
        "https://xcancel.com",
    ]
    
    def __init__(self, proxy: Optional[str] = None):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                         'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ä»£ç†è®¾ç½®
        self.proxies = None
        
        # 1. ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¼ å…¥çš„ä»£ç†
        if proxy:
            self.proxies = {'http': proxy, 'https': proxy}
            print(f"ğŸ”Œ ä½¿ç”¨æŒ‡å®šä»£ç†: {proxy}")
        # 2. å…¶æ¬¡å°è¯•è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿä»£ç†
        else:
            sys_proxy = self._detect_system_proxy()
            if sys_proxy:
                self.proxies = {'http': sys_proxy, 'https': sys_proxy}
                print(f"ğŸ”Œ è‡ªåŠ¨æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†: {sys_proxy}")
        
        if self.proxies:
            self.session.proxies = self.proxies

    def _detect_system_proxy(self) -> Optional[str]:
        """æ£€æµ‹ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†è®¾ç½®"""
        # å¸¸è§çš„ä»£ç†ç¯å¢ƒå˜é‡
        keys = ['ALL_PROXY', 'all_proxy', 'HTTPS_PROXY', 'https_proxy', 'HTTP_PROXY', 'http_proxy']
        for key in keys:
            val = os.environ.get(key)
            if val:
                # ç¡®ä¿æœ‰åè®®å‰ç¼€
                if not val.startswith('http'):
                    return f"http://{val}"
                return val
        return None

    def extract(self, url: str) -> Optional[Dict]:
        """æå–æ¨æ–‡å†…å®¹"""
        # æå– tweet ID
        match = re.search(r'(?:twitter|x)\.com/([^/]+)/status/(\d+)', url)
        if not match:
            # å°è¯•å¤„ç†çŸ­é“¾æ¥æˆ–ä¸è§„èŒƒé“¾æ¥
            match = re.search(r'/status/(\d+)', url)
            
        if not match:
            print(f"âŒ æ— æ³•è¯†åˆ«çš„ Twitter é“¾æ¥: {url}")
            return None
            
        tweet_id = match.group(2) if len(match.groups()) > 1 else match.group(1)
        
        # 1. å°è¯• Guest Token (API)
        # print("â³ å°è¯•ç›´æ¥è¿æ¥ X API (Guest Mode)...")
        guest_data = self._extract_via_guest_token(tweet_id)
        if guest_data:
            return guest_data

        # 2. å°è¯• Nitter å®ä¾‹
        # print("âš ï¸ X API è¿æ¥å¤±è´¥ï¼Œå°è¯• Nitter ä»£ç†...")
        for instance in self.NITTER_INSTANCES:
            try:
                # éœ€è¦ usernameï¼Œå¦‚æœæ­£åˆ™æ²¡å–åˆ°ï¼Œéšä¾¿å¡«ä¸€ä¸ªï¼ŒNitter é€šå¸¸èƒ½é‡å®šå‘æˆ–å…¼å®¹
                username = "user"
                nitter_url = f"{instance}/{username}/status/{tweet_id}"
                
                # ç¼©çŸ­è¶…æ—¶æ—¶é—´ï¼Œå¿«é€Ÿå¤±è´¥
                response = self.session.get(nitter_url, timeout=5)
                
                if response.status_code == 200:
                    post_data = self._parse_nitter_page(response.text, url, instance)
                    if post_data:
                        return post_data
            except Exception:
                continue

        # 3. Syndication API (ä¿åº•)
        try:
            syndication_url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&lang=en"
            response = self.session.get(syndication_url, timeout=5)
            if response.status_code == 200:
                return self._parse_syndication_response(response.json(), url)
        except Exception:
            pass

        return None

    def _get_guest_token(self) -> Optional[str]:
        headers = {
            "authorization": f"Bearer {self.GUEST_BEARER_TOKEN}",
            "user-agent": self.headers['User-Agent']
        }
        try:
            response = requests.post(
                "https://api.twitter.com/1.1/guest/activate.json", 
                headers=headers, 
                proxies=self.proxies,
                timeout=5
            )
            if response.status_code == 200:
                return response.json()["guest_token"]
        except Exception:
            pass
        return None

    def _extract_via_guest_token(self, tweet_id: str) -> Optional[Dict]:
        guest_token = self._get_guest_token()
        if not guest_token:
            return None
            
        url = "https://twitter.com/i/api/graphql/s-CskcDsK2j0Nq5X6yV6lA/TweetResultByRestId"
        headers = {
            "authorization": f"Bearer {self.GUEST_BEARER_TOKEN}",
            "x-guest-token": guest_token,
            "content-type": "application/json",
            "user-agent": self.headers['User-Agent']
        }
        
        variables = {
            "tweetId": tweet_id,
            "withCommunity": False,
            "includePromotedContent": False,
            "withVoice": False
        }
        
        # ç®€åŒ–çš„ features
        features = {
            "creator_subscriptions_tweet_preview_api_enabled": True,
            "longform_notetweets_consumption_enabled": True,
            "tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled": True,
            "longform_notetweets_rich_text_read_enabled": True,
            "longform_notetweets_inline_media_enabled": True,
        }
        
        params = {
            "variables": json.dumps(variables),
            "features": json.dumps(features)
        }
        
        try:
            response = requests.get(url, headers=headers, params=params, proxies=self.proxies, timeout=10)
            if response.status_code == 200:
                data = response.json()
                return self._parse_guest_api_data(data, tweet_id)
        except Exception:
            pass
        return None

    def _parse_guest_api_data(self, data: Dict, tweet_id: str) -> Optional[Dict]:
        try:
            result = data['data']['tweetResult']['result']
            legacy = result['legacy']
            core = result['core']['user_results']['result']
            
            # æ–‡æœ¬
            text = legacy.get('full_text', '')
            
            # ä½œè€…
            author = {
                'name': core['legacy']['name'],
                'username': core['legacy']['screen_name']
            }
            
            # åª’ä½“
            media = []
            if 'extended_entities' in legacy and 'media' in legacy['extended_entities']:
                for m in legacy['extended_entities']['media']:
                    media.append({
                        'type': m.get('type', 'photo'),
                        'url': m.get('media_url_https', '')
                    })
            
            return {
                'text': text,
                'author': author,
                'media': media,
                'quoted_tweet': None # æš‚ä¸å¤„ç†åµŒå¥—å¼•ç”¨ï¼Œä¿æŒç®€å•
            }
        except Exception:
            return None

    def _parse_nitter_page(self, html: str, original_url: str, instance_url: str) -> Optional[Dict]:
        soup = BeautifulSoup(html, 'html.parser')
        tweet_container = soup.find('div', class_='main-tweet')
        if not tweet_container:
            return None
            
        content_div = tweet_container.find('div', class_='tweet-content')
        text = content_div.get_text(separator='\n').strip() if content_div else ""
        
        author_name = tweet_container.find('a', class_='fullname')
        author_user = tweet_container.find('a', class_='username')
        
        author = {
            'name': author_name.get_text().strip() if author_name else "",
            'username': author_user.get_text().strip().replace('@', '') if author_user else ""
        }
        
        media = []
        attachments = tweet_container.find('div', class_='attachments')
        if attachments:
            for img in attachments.find_all('img'):
                src = img.get('src', '')
                if src:
                    if src.startswith('/'): src = instance_url + src
                    media.append({'type': 'photo', 'url': src})
                    
        return {'text': text, 'author': author, 'media': media}

    def _parse_syndication_response(self, data: Dict, url: str) -> Dict:
        text = data.get('text', '')
        author = {
            'name': data.get('user', {}).get('name', ''),
            'username': data.get('user', {}).get('screen_name', '')
        }
        media = []
        for m in (data.get('entities', {}).get('media', []) or []):
            media.append({'type': m.get('type', ''), 'url': m.get('media_url_https', '')})
            
        return {'text': text, 'author': author, 'media': media}

    def format_output(self, data: Dict) -> str:
        """åªè¾“å‡ºå†…å®¹ï¼Œä¸å«å…ƒæ•°æ®"""
        output = []
        
        # 1. å¤´éƒ¨ï¼šä½œè€…ä¿¡æ¯ (è¿˜æ˜¯ä¿ç•™ä¸€ä¸‹æ¯”è¾ƒå¥½ï¼ŒçŸ¥é“æ˜¯è°è¯´çš„)
        # output.append(f"**{data['author']['name']}** (@{data['author']['username']}):")
        # output.append("")
        
        # 2. æ­£æ–‡
        output.append(data['text'])
        
        # 3. åª’ä½“
        if data['media']:
            output.append("")
            for m in data['media']:
                if m['type'] == 'photo':
                    output.append(f"![image]({m['url']})")
                else:
                    output.append(f"[Media: {m['url']}]")
                    
        return "\n".join(output)

def main():
    parser = argparse.ArgumentParser(description='X/Twitter å†…å®¹æå–å™¨')
    parser.add_argument('urls', nargs='+', help='X/Twitter é“¾æ¥')
    parser.add_argument('-p', '--proxy', help='ä»£ç†åœ°å€')
    args = parser.parse_args()
    
    extractor = TwitterExtractor(proxy=args.proxy)
    
    for url in args.urls:
        data = extractor.extract(url)
        if data:
            print(extractor.format_output(data))
        else:
            print(f"âŒ æå–å¤±è´¥: {url} (è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®)")

if __name__ == '__main__':
    main()
